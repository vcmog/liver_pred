{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Cross-Sectional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# my packaes\n",
    "import utils.colonflag.feature_generation as fg\n",
    "import utils.missing_data as md\n",
    "# data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# evaluation\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " #confusion_matrix, ConfusionMatrixDisplay,roc_auc_score, average_precision_score, f1_score, classification_report, precision_score, recall_score, roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "#utils\n",
    "import joblib\n",
    "from sys import getsizeof\n",
    "\n",
    "#from scipy.stats import linregress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r\"C:\\Users\\victo\\OneDrive - University of Leeds\\Documents\\Uni Work\\Project\\MIMIC Work\\Liver Cancer Prediction\\liver_pred\\data\\interim\"\n",
    "model_dir = r\"C:\\Users\\victo\\OneDrive - University of Leeds\\Documents\\Uni Work\\Project\\MIMIC Work\\Liver Cancer Prediction\\liver_pred\\data\\models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cohort_ids = pd.read_csv(dir + r\"\\matched_cohort_ids.csv\", index_col=0)\n",
    "processed_labs = pd.read_csv(dir + r\"\\processed_lab_data.csv\", parse_dates=[\"charttime\", \"index_date\"], index_col=0,\n",
    "                             usecols=['subject_id',  'index_admission',  'test_admission', 'itemid', 'valuenum', 'charttime', 'outcome', 'index_date', 'label'],\n",
    "                             dtype = {'subject_id':int, 'index_admission':int, 'test_admission': pd.Int32Dtype(), 'itemid':int,\n",
    "                                      'valuenum': 'float64','outcome': bool,'label': 'category'})\n",
    "# pd.Int32Dtype() is used to allow for NaN values in the test_admission column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(feature_df):\n",
    "\n",
    "    print(\"Preparing Features\")\n",
    "    feature_df = feature_df.fillna(0, inplace=False)\n",
    "    y = feature_df['outcome']\n",
    "    X = feature_df.drop(columns=[\"outcome\"])\n",
    "    X_train, X_test,y_train, y_test = train_test_split(X, y, train_size = 0.8)\n",
    "    print(f\"Train Length: {len(X_train)}        Train cases: {len(y_train[y_train==1])}    Proportion: {len(y_train[y_train==1])/len(y_train)*100} %\")\n",
    "    print(f\"Test Length: {len(X_test)}          Test cases: {len(y_test[y_test==1])}       Proportion: {len(y_test[y_test==1])/len(y_test)*100} %\")\n",
    "\n",
    "    #### Scale training #####\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "    X_train_scaled = X_train_scaled.set_axis(X_train.columns,axis=1)\n",
    "\n",
    "    #### Scale Test using training scaler ####\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "    X_test_scaled.set_axis(X_test.columns,axis=1)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_probs, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a binary classification model by calculating various metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_probs (array-like): Predicted probabilities for the positive class.\n",
    "    - y_true (array-like): True labels for the samples.\n",
    "\n",
    "    Returns:\n",
    "    - f1 (float): F1 score.\n",
    "    - roc_auc (float): ROC AUC score.\n",
    "    - precision (float): Precision score.\n",
    "    - recall (float): Recall score.\n",
    "    \"\"\"\n",
    "    y_pred = y_probs > 0.5\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    roc_auc = metrics.roc_auc_score(y_true, y_probs)\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    print(f\"Precision: {precision}\")\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    print(f\"Recall: {recall}\")\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\", confusion_matrix)\n",
    "    return f1, roc_auc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    print(f\"Running {model}\")\n",
    "    clf = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, model_dir+r\"\\bestparams_\" + f\"{model}.pkl\")\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_prob = clf.predict_proba(X_train)[:,1]\n",
    "    return y_pred, y_prob, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models on *training set only* with no lead time to find optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'LR':LogisticRegression(), 'RF':RandomForestClassifier(), 'GB': GradientBoostingClassifier(), 'NN': MLPClassifier(), 'SVM':SVC(probability=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Features\n",
      "Train Length: 6251        Train cases: 1087    Proportion: 17.389217725163974 %\n",
      "Test Length: 1563          Test cases: 271       Proportion: 17.338451695457454 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\OneDrive - University of Leeds\\Documents\\Uni Work\\Project\\MIMIC Work\\Liver Cancer Prediction\\liver_pred\\liver_pred\\utils\\colonflag\\feature_generation.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  current = pd.concat([current, missing_data])\n"
     ]
    }
   ],
   "source": [
    "lab_df_3day = fg.current_bloods_df(processed_labs,lead_time=0, n_days_pre=3, n_days_post=1)\n",
    "X_train, _, y_train, _ = prepare_features(lab_df_3day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for logistic regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LogisticRegression()\n",
      "F1 Score: 0.5380184331797235\n",
      "ROC AUC: 0.8747077460046448\n",
      "Precision: 0.7195685670261941\n",
      "Recall: 0.4296228150873965\n",
      "Confusion Matrix: [[4982  182]\n",
      " [ 620  467]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_prob, lr = tune_hyperparameters(models['LR'], param_grid_lr, X_train, y_train)\n",
    "results = evaluate_performance(y_prob, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for random forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_hyperparameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred, y_prob, RF \u001b[38;5;241m=\u001b[39m tune_hyperparameters(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m], param_grid_rf, X_train, y_train)\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_performance(y_prob, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tune_hyperparameters' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred, y_prob, RF = tune_hyperparameters(models['RF'], param_grid_rf, X_train, y_train)\n",
    "results = evaluate_performance(y_prob, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for gradient boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_hyperparameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred, y_prob, GB \u001b[38;5;241m=\u001b[39m tune_hyperparameters(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m'\u001b[39m], param_grid_gb, X_train, y_train)\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_performance(y_prob, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tune_hyperparameters' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred, y_prob, GB = tune_hyperparameters(models['GB'], param_grid_gb, X_train, y_train)\n",
    "results = evaluate_performance(y_prob, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for neural network\n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_prob, NN = tune_hyperparameters(models['NN'], param_grid_nn, X_train, y_train)\n",
    "results = evaluate_performance(y_prob, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: SVM doesn't estimate probabilities, that's done using validation. Adjust the evaluation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_prob, SVM = tune_hyperparameters(models['SVM'], param_grid_svc, X_train, y_train)\n",
    "results = evaluate_performance(y_prob, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = joblib.load(model_dir+r\"\\bestparams_LogisticRegression().pkl\")\n",
    "#rf_best = joblib.load(model_dir+r\"\\bestparams_RandomForestClassifier().pkl\")\n",
    "#gb_best = joblib.load(model_dir+r\"\\bestparams_GradientBoostingClassifier().pkl\")\n",
    "#nn_best = joblib.load(model_dir+r\"\\bestparams_MLPClassifier().pkl\")\n",
    "#svm_best = joblib.load(model_dir+r\"\\bestparams_SVC(probability=True).pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model with different lead times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lead_times(lead_times, processed_labs):\n",
    "\n",
    "    lr_aucs = np.zeros(len(lead_times))\n",
    "    lr_precisions = np.zeros(len(lead_times))\n",
    "    lr_recalls = np.zeros(len(lead_times))\n",
    "    n_iter = 0\n",
    "    iter_times = np.zeros(len(lead_times))\n",
    "    for l_time in lead_times:\n",
    "        lab_df = fg.current_bloods_df(processed_labs, lead_time = l_time, n_days_pre=3, n_days_post=1)\n",
    "        X_train, _, y_train, _ = prepare_features(lab_df)\n",
    "        lab_df = None\n",
    "        results = cross_validate(lr_best, X_train, y_train, scoring = ('roc_auc', 'precision', 'recall'), cv=5, n_jobs=2)\n",
    "        X_train = y_train = None\n",
    "        lr_aucs[n_iter] = results['test_roc_auc'].mean()\n",
    "        lr_precisions[n_iter] = results['test_precision'].mean()\n",
    "        lr_recalls[n_iter] = results['test_recall'].mean()\n",
    "        iter_time = sum(results['fit_time'])+sum(results['score_time'])\n",
    "        iter_times[n_iter] = iter_time\n",
    "        print(\"Iter:\", n_iter+1, \"Iter time:\", iter_time, \"Estimated Total:\", sum(iter_times)/len(iter_times)*len(lead_times)/60, \"mins\")\n",
    "        n_iter +=1\n",
    "    return lr_aucs, lr_precisions, lr_recalls, iter_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_times = range(0,735,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_aucs, lr_precisions, lr_recalls, iter_times = evaluate_lead_times(lead_times, processed_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot LR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUCs\n",
    "plt.plot(lead_times, lr_aucs, label='Logistic Regression')\n",
    "plt.xlabel('Lead Time (days)')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot precisions\n",
    "plt.plot(lead_times, lr_precisions, label='Logistic Regression')\n",
    "plt.xlabel('Lead Time (days)')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot recalls\n",
    "plt.plot(lead_times, lr_recalls, label='Logistic Regression')\n",
    "plt.xlabel('Lead Time (days)')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIVER_PRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
